<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <!-- <meta property="og:url" content="URL OF THE WEBSITE"/> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-Language Models, Culture Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>CultureVLM</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <!-- <span class="author-block">
                <a href="https://sudanl.github.io" target="_blank">Shudong Liu</a></span>
                <span class="author-block">
                  <a href="https://ahren09.github.io/" target="_blank">Yiqiao Jin</a><sup>*</sup>,</span> -->
                  <span class="author-block">
                    <a href="https://sudanl.github.io" target="_blank">Shudong Liu</a><sup>1</sup>&emsp;
                  </span>
                  <span class="author-block">
                      <a href="https://ahren09.github.io/" target="_blank">Yiqiao Jin</a><sup>2</sup>&emsp;
                  </span>
                  <span class="author-block">
                    <a href="" target="_blank">Cheng Li</a><sup>3</sup>&emsp;
                  </span>
                  <span class="author-block">
                    <a href="https://www.fst.um.edu.mo/personal/derek-wong" target="_blank">Derek F. Wong</a><sup>1</sup>&emsp;
                  </span>
                  <br/>
                  <span class="author-block">
                    <a href="https://sites.google.com/site/qingsongwen8" target="_blank">Qingsong Wen</a><sup>4</sup>&emsp;
                  </span>
                  <span class="author-block">
                    <a href="https://lichao-sun.github.io" target="_blank">Lichao Sun</a><sup>5</sup>&emsp;
                  </span>
                  <span class="author-block">
                    <a href="https://haipeng-chen.github.io/" target="_blank">Haipeng Chen</a><sup>6</sup>&emsp;
                  </span>
                  <span class="author-block">
                    <a href="https://www.microsoft.com/en-us/research/people/xingx/ " target="_blank">Xing Xie</a><sup>3</sup>&emsp;
                  </span>
                  <span class="author-block">
                    <a href="https://jd92.wang/" target="_blank">Jindong Wang</a><sup>6</sup>&emsp;
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <sup>1</sup>University of Macau&emsp;
                    <sup>2</sup>Georgia Institute of Technology&emsp;
                    <sup>3</sup>Microsoft Research&emsp;
                    <br/>
                    <sup>4</sup>Squirrel AI&emsp;
                    <sup>5</sup>Lehigh University&emsp;
                    <sup>6</sup>William & Mary&emsp;
                    <!-- <span class="author-block">Institution Name<br>Conferance name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2501.01282" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Data (Soon)</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2501.01282" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video> -->
      <!-- 换成pdf static/images/fig-dataset.pdf -->
      <!-- <!-- <iframe  src="static/pdfs/fig-dataset.pdf" width="100%" height="550"> -->
      <img src="static/images/fig-dataset.jpg" alt="MY ALT TEXT"/>

      <h2 class="subtitle has-text-centered">
        Overview of CultureVerse. In total, there are over 220k instances and 19k cultural concepts for training and evaluation, respectively, composed of 3 different types of questions from 188 countries.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin ullamcorper tellus sed ante aliquam tempus. Etiam porttitor urna feugiat nibh elementum, et tempor dolor mattis. Donec accumsan enim augue, a vulputate nisi sodales sit amet. Proin bibendum ex eget mauris cursus euismod nec et nibh. Maecenas ac gravida ante, nec cursus dui. Vivamus purus nibh, placerat ac purus eget, sagittis vestibulum metus. Sed vestibulum bibendum lectus gravida commodo. Pellentesque auctor leo vitae sagittis suscipit. -->
            Vision-language models (VLMs) have advanced human-AI interaction but struggle with cultural understanding, often misinterpreting symbols, gestures, and artifacts due to biases in predominantly Western-centric training data. In this paper, we construct CultureVerse, a large-scale multimodal benchmark covering 19, 682 cultural concepts, 188countries/regions, 15 cultural concepts, and 3 question types, with the aim of characterizing and improving VLMs’
            multicultural understanding capabilities. Then, we propose CultureVLM, a series of VLMs fine-tuned on our dataset to achieve significant performance improvement in cultural understanding. Our evaluation of 16 models reveals significant disparities, with a stronger performance in Western concepts and weaker results in African and Asian contexts. Fine-tuning on our CultureVerse enhances cultural perception, demonstrating cross-cultural, cross-continent,  and cross-dataset generalization without sacrificing performance on models’ general VLM benchmarks. We further present insights on cultural generalization and forgetting. We hope that this work could lay the foundation for more equitable and culturally aware multimodal AI systems.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
  <h2 class="content has-text-justified">
    <h2 class="title is-3">Data Construction Pipeline</h2>
    <img src="static/images/pipeline.jpg" height="100%"/>
    <h2 class="content has-text-justified">
      Our pipeline to build CultureVerse and CultureVLM, including Tangible Cultural Concept Collection, Question-Answer Generation, Quality Assurance and Training. Our approach to constructing multimodal cultural datasets is notably more scalable and comprehensive than existing methods.
    </h2>
</div>
</div>
</div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
  <h2 class="content has-text-justified">
    <h2 class="title is-3">Performance</h2>
    <img src="static/images/performance.jpg" height="100%"/>
    <h2 class="content has-text-justified">
<!--       Task Difficulty: Cultural Scene Reasoning Outperforms Recognition. 
      Regional Disparities: Better Performance in Western Cultures.
      Weak Understanding of History and Landmarks.
      Performance Variability from Model Level.
      Direct Answer v.s. Stepwise Reasoning. 五个list -->
      <ul>
        <li>Task Difficulty: Cultural Scene Reasoning Outperforms Recognition.</li>
        <li>Regional Disparities: Better Performance in Western Cultures.</li>
        <li>Weak Understanding of History and Landmarks.</li>
        <li>Performance Variability from Model Level: Although larger models tend to demonstrate better performance, size alone is not the determining factor</li>
        <li>Direct Answer v.s. Stepwise Reasoning: stepwise reasoning does not improve cultural recognition and, in most cases, significantly impair performance</li>
      </ul>
    </h2>
</div>
</div>
</div>
</section>


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{liu2025culturevlm,
  title={CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries},
  author={Liu, Shudong and Jin, Yiqiao and Li, Cheng and Wong, Derek F and Wen, Qingsong and Sun, Lichao and Chen, Haipeng and Xie, Xing and Wang, Jindong},
  journal={arXiv preprint arXiv:2501.01282},
  year={2025}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
